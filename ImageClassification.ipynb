{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled19.ipynb",
      "provenance": [],
      "mount_file_id": "1X1IQZmFsV3vyiHNjEtA3K357Yzzw-Nr-",
      "authorship_tag": "ABX9TyMDJgtds0/wxXQaaV6zLz16",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShikhaVishwakarma/EagleView-DL-ML-Assignment-for-Data-Scientist/blob/main/ImageClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nff8514Gdg4u"
      },
      "source": [
        "#Getting started\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYWS06CIYlKn"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os.path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from keras.preprocessing.image import load_img, img_to_array"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "843OpNcswUTm"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa01wCrgcvFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3727ee0-4ee9-44d4-ebee-72fef41f357b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm3w27c230pT"
      },
      "source": [
        "image_dir=Path('/content/gdrive/MyDrive/CarPerson')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYBJ775UdrM5"
      },
      "source": [
        "#Create file dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHMaA8YUVbtv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "c796ab20-b505-4820-b514-e79c53872b81"
      },
      "source": [
        "filepaths=list(image_dir.glob(r'**/*.jpg'))\n",
        "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
        "\n",
        "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
        "labels = pd.Series(labels, name='Label')\n",
        "image_df = pd.concat([filepaths, labels], axis=1)\n",
        "image_df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filepath</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/gdrive/MyDrive/CarPerson/Person/image...</td>\n",
              "      <td>Person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/gdrive/MyDrive/CarPerson/Person/image...</td>\n",
              "      <td>Person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/gdrive/MyDrive/CarPerson/Person/image...</td>\n",
              "      <td>Person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/gdrive/MyDrive/CarPerson/Person/image...</td>\n",
              "      <td>Person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/gdrive/MyDrive/CarPerson/Person/image...</td>\n",
              "      <td>Person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2234</th>\n",
              "      <td>/content/gdrive/MyDrive/CarPerson/Car/image_00...</td>\n",
              "      <td>Car</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2235</th>\n",
              "      <td>/content/gdrive/MyDrive/CarPerson/Car/image_00...</td>\n",
              "      <td>Car</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2236</th>\n",
              "      <td>/content/gdrive/MyDrive/CarPerson/Car/image_00...</td>\n",
              "      <td>Car</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2237</th>\n",
              "      <td>/content/gdrive/MyDrive/CarPerson/Car/image_00...</td>\n",
              "      <td>Car</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2238</th>\n",
              "      <td>/content/gdrive/MyDrive/CarPerson/Car/image_00...</td>\n",
              "      <td>Car</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2239 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Filepath   Label\n",
              "0     /content/gdrive/MyDrive/CarPerson/Person/image...  Person\n",
              "1     /content/gdrive/MyDrive/CarPerson/Person/image...  Person\n",
              "2     /content/gdrive/MyDrive/CarPerson/Person/image...  Person\n",
              "3     /content/gdrive/MyDrive/CarPerson/Person/image...  Person\n",
              "4     /content/gdrive/MyDrive/CarPerson/Person/image...  Person\n",
              "...                                                 ...     ...\n",
              "2234  /content/gdrive/MyDrive/CarPerson/Car/image_00...     Car\n",
              "2235  /content/gdrive/MyDrive/CarPerson/Car/image_00...     Car\n",
              "2236  /content/gdrive/MyDrive/CarPerson/Car/image_00...     Car\n",
              "2237  /content/gdrive/MyDrive/CarPerson/Car/image_00...     Car\n",
              "2238  /content/gdrive/MyDrive/CarPerson/Car/image_00...     Car\n",
              "\n",
              "[2239 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1VVw05bqKRJ"
      },
      "source": [
        "train_df, test_df = train_test_split(image_df, train_size=0.8, shuffle=True, random_state=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XjCTb4Sdw2q"
      },
      "source": [
        "#Load image data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtmD6Dj4q-3m"
      },
      "source": [
        "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    horizontal_flip=True,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg4agSZtrIBZ",
        "outputId": "63864f81-5c57-471b-bb28-e81163771432"
      },
      "source": [
        "train_images = train_generator.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(224, 224),\n",
        "    color_mode='rgb',\n",
        "    class_mode='binary',\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    subset='training'\n",
        ")\n",
        "val_images = train_generator.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(224, 224),\n",
        "    color_mode='rgb',\n",
        "    class_mode='binary',\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    subset='validation'\n",
        ")\n",
        "test_images = test_generator.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(224, 224),\n",
        "    color_mode='rgb',\n",
        "    class_mode='binary',\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1433 validated image filenames belonging to 2 classes.\n",
            "Found 358 validated image filenames belonging to 2 classes.\n",
            "Found 448 validated image filenames belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjgUQ899E8MV"
      },
      "source": [
        "#Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXS5nY4jE4dm",
        "outputId": "c55e2390-55e6-444f-a81a-699de5af86a1"
      },
      "source": [
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "x = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(inputs)\n",
        "x = tf.keras.layers.MaxPool2D()(x)\n",
        "x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPool2D()(x)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_images,\n",
        "    validation_data=val_images,\n",
        "    epochs=100,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            patience=3\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "45/45 [==============================] - 271s 6s/step - loss: 0.6948 - accuracy: 0.5115 - val_loss: 0.6939 - val_accuracy: 0.4888 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "45/45 [==============================] - 52s 1s/step - loss: 0.6935 - accuracy: 0.4885 - val_loss: 0.6934 - val_accuracy: 0.4888 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "45/45 [==============================] - 52s 1s/step - loss: 0.6935 - accuracy: 0.4857 - val_loss: 0.6932 - val_accuracy: 0.4860 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "45/45 [==============================] - 51s 1s/step - loss: 0.6932 - accuracy: 0.5031 - val_loss: 0.6933 - val_accuracy: 0.4888 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "45/45 [==============================] - 51s 1s/step - loss: 0.6934 - accuracy: 0.5031 - val_loss: 0.6933 - val_accuracy: 0.4888 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "45/45 [==============================] - 51s 1s/step - loss: 0.6932 - accuracy: 0.5031 - val_loss: 0.6934 - val_accuracy: 0.4888 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "45/45 [==============================] - 51s 1s/step - loss: 0.6931 - accuracy: 0.5031 - val_loss: 0.6934 - val_accuracy: 0.4888 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "45/45 [==============================] - 51s 1s/step - loss: 0.6931 - accuracy: 0.5031 - val_loss: 0.6933 - val_accuracy: 0.4888 - lr: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgasKxaDKHO-",
        "outputId": "5ca18204-7feb-4657-97f9-d4c965c9d8c6"
      },
      "source": [
        "train_images.class_indices"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Car': 0, 'Person': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egBgSl1Ad70H"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkxM2CUHIfzy",
        "outputId": "e9eb6f41-5678-48dd-ceca-63f6df241d65"
      },
      "source": [
        "results = model.evaluate(test_images, verbose=0)\n",
        "\n",
        "print(\"    Test Loss: {:.5f}\".format(results[0]))\n",
        "print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Test Loss: 0.69316\n",
            "Test Accuracy: 49.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "l0SNrWyQJiD4",
        "outputId": "4bdbab78-815f-496b-ba73-e2e228b395ad"
      },
      "source": [
        "predictions = (model.predict(test_images) >= 0.5).astype(np.int)\n",
        "\n",
        "cm = confusion_matrix(test_images.labels, predictions, labels=[0, 1])\n",
        "clr = classification_report(test_images.labels, predictions, labels=[0, 1], target_names=[\"CAR\", \"PERSON\"])\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)\n",
        "plt.xticks(ticks=[0.5, 1.5], labels=[\"CAR\", \"PERSON\"])\n",
        "plt.yticks(ticks=[0.5, 1.5], labels=[\"CAR\", \"PRESON\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Classification Report:\\n----------------------\\n\", clr)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAGDCAYAAAAoI6sGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAbnElEQVR4nO3dd5hdZbn38e+dBoEQSAIJTUpQwAASikdAQVBeBQQponR5BYxUQYGDQg6IoOARECmKSpEiELgiXQwepOORBEzA0KVLkJ5CDcl9/thrwmbITIaYNcPM8/1c11ys9azy3HvY+c3azyo7MhNJUs/Xq6sLkCR1DgNfkgph4EtSIQx8SSqEgS9JhTDwJakQBr56jIjoHxHXRMTUiLj839jPbhFxw4KsrStExPURsWdX16EPDwNfnS4ido2ICRExIyKmVMH0mQWw6x2BYcCQzPzq/O4kM3+XmV9YAPW8R0RsGhEZEVe0al+7ar+5g/v5QURcNK/1MnPLzDx/PstVD2Tgq1NFxHeBU4Ef0wjnFYBfANsugN2vCDycme8sgH3V5QVgw4gY0tS2J/DwguogGvy3rffxTaFOExGLAz8EDsjM32fma5k5MzOvyczDq3UWiohTI+LZ6ufUiFioWrZpRDwTEYdGxPPVp4NvVMuOBY4Gdqo+Oezd+kg4IlaqjqT7VPP/PyIei4jpEfF4ROzW1H5703YbRcT4aqhofERs1LTs5og4LiLuqPZzQ0Qs2c6v4W3gSmDnavvewE7A71r9rn4eEU9HxLSIuDsiNq7atwCObHqdk5rq+FFE3AG8Dgyv2vaplv8yIsY27f8nEXFjRESH/weq2zPw1Zk2BBYGrmhnnaOADYCRwNrAfwCjm5YvDSwOLAfsDZwZEYMy8xganxrGZOaAzDynvUIiYlHgNGDLzFwM2AiYOJf1BgPXVesOAU4Brmt1hL4r8A1gKNAPOKy9voELgK9X018E/g4822qd8TR+B4OBi4HLI2LhzPxjq9e5dtM2ewCjgMWAJ1vt71BgreqP2cY0fnd7ps9WKYqBr840BHhxHkMuuwE/zMznM/MF4FgaQdZiZrV8Zmb+AZgBrDaf9cwG1oyI/pk5JTMnz2WdLwGPZOaFmflOZl4CPAhs07TOeZn5cGa+AVxGI6jblJl3AoMjYjUawX/BXNa5KDNfqvo8GViIeb/O32bm5Gqbma329zqN3+MpwEXAQZn5zDz2px7GwFdneglYsmVIpQ3L8t6j0yertjn7aPUH43VgwActJDNfozGUsi8wJSKui4jVO1BPS03LNc0/Nx/1XAgcCGzGXD7xRMRhEfFANYz0Ko1PNe0NFQE83d7CzPwr8BgQNP4wqTAGvjrTX4C3gO3aWedZGidfW6zA+4c7Ouo1YJGm+aWbF2bmuMz8f8AyNI7af9OBelpq+ud81tTiQmB/4A/V0fcc1ZDLfwJfAwZl5hLAVBpBDdDWMEy7wzMRcQCNTwrPVvtXYQx8dZrMnErjxOqZEbFdRCwSEX0jYsuI+O9qtUuA0RGxVHXy82gaQxDzYyKwSUSsUJ0w/n7LgogYFhHbVmP5b9EYGpo9l338AVi1upS0T0TsBIwArp3PmgDIzMeBz9I4Z9HaYsA7NK7o6RMRRwMDm5b/C1jpg1yJExGrAscDu9MY2vnPiGh36Ek9j4GvTlWNR3+XxonYF2gMQxxI48oVaITSBOBe4D7gnqptfvr6EzCm2tfdvDeke1V1PAu8TCN895vLPl4CtqZx0vMlGkfGW2fmi/NTU6t9356Zc/v0Mg74I41LNZ8E3uS9wzUtN5W9FBH3zKufagjtIuAnmTkpMx+hcaXPhS1XQKkM4Ul6SSqDR/iSVAgDX5IKYeBLUiEMfEkqhIEvSYVo747HLtV/w+95+ZA+lJ4Yd1xXlyC1adjAvm0+EM8jfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiE6PfAjYoXO7lOSVGPgR8SGEbFjRAyt5j8RERcDd9TVpySpbbUEfkT8FDgX+ApwXUQcD9wA/BX4WB19SpLa16em/X4JWCcz34yIQcDTwJqZ+URN/UmS5qGuwH8zM98EyMxXIuIRw74+yw9dnLOP/hpDBw8gE8696i7OvOwOBg3sz4XH7cqKywziySmvsPvoi3l1+htsvM5wLv/vr/PEsy8DcNUtkznh3Bu7+FWoBCf+cDR33n4rgwYN5vwxV85pHzvmd1xx+aX06tWLDT+zCft9+9AurLLnqivwh0fE1U3zKzfPZ+aXa+q3SO/Mms33TruOiQ8/y4BF+nHneQdx412PsMeX1uPmCY9y0oW3cNgen+WwPT7L6F/8EYA7Jj3OVw47v4srV2m22Ho7tv/arvz4mCPntN0z4S5uv+Umzr14LP369eOVl1/qwgp7troCf9tW8yfX1I+A516aznMvTQdgxutv8+ATL7DsUgPZeuMRfPGAXwNw0R/uYdyZo+YEvtQVRq67PlOe/ed72q4aO4bd9tybfv36ATBo8JCuKK0ItQR+Zt4yt/aI+AiwMzDX5fr3rbD0IEauuizjJz/N0MED5vwheO6l6QwdPGDOep9acwX+esHBTHlxGt8//ToeePz5ripZhXv6ySe4d+Ld/OaXp9Gv30Lsf/ChfHyNtbq6rB6p9uvwI2KpiNg/Im4DbgaGtbPuqIiYEBET3vnXxLpL63EW7d+PS07YjcNPvYbpr7/1vuWZjf9OfOifrLb9T/jU13/OLy+/k8t+8vVOrlR616xZs5g2bRpnnXcx+x18KMcceRjZ8mbVAlXXZZmLRcSeETEOuAtYBVg5M1fJzMPa2i4zf52Z62fm+n2GjayjtB6rT+9eXPLj3RkzbiJX3TIZgOdfnsHSQxYDYOkhi/HCKzMAmP76W7z2xtsAjPvLQ/Tt05shiy/SNYWreEsNHcYmm21ORDBijbXoFcHUV1/p6rJ6pLqO8J8H9gKOB4Zn5qHA2zX1JeCso3bkoSef57RLb5/Tdt3t97P7VusCsPtW63LtbfcDMKxpaGf9EcvTK4KXpr7euQVLlY03/Rx/m3AX0BjemTlzJosvMaiLq+qZoo6PThFxCI2x+kWBS4AxwJ8yc3hH99F/w+/5ma6DNvrEitz4q/2479EpzJ7d+LUdc9Y4xk9+mot+tCsfGbYETz3XuCzzlWlvsO+OG/LN7TfgnVmzefOtmRxx2rX8731PdfGr6D6eGHdcV5fQbR171OH87e7xTH31VQYPGcI3Ru3PF7f6Mif+cDSPPvwQffr2Zf+DD2O9T36qq0vttoYN7BttLasl8OfsPGI4jeDfhcYdtkcDV2bmw/Pa1sDXh5WBrw+z9gK/rjH8j0bEpzPzscz8cWauBXwS2AJ4oI4+JUntq2sM/1RgWnNDZt4HHAJcX1OfkqR21BX4w6qAf4/MvBdYsaY+JUntqCvwl2hnWf+a+pQktaOuwJ8QEd9s3RgR+wB319SnJKkddT1L5xDgiojYjXcDfn2gH7B9TX1KktpR17N0/gVsFBGbAWtWzddl5p/r6E+SNG91HeEDkJk3ATfV2YckqWM6/UvMJUldw8CXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQfdpaEBGnA9nW8sz8di0VSZJq0WbgAxM6rQpJUu3aDPzMPL8zC5Ek1au9I3wAImIp4AhgBLBwS3tmfq7GuiRJC1hHTtr+DngAWBk4FngCGF9jTZKkGnQk8Idk5jnAzMy8JTP3Ajy6l6RuZp5DOsDM6r9TIuJLwLPA4PpKkiTVoSOBf3xELA4cCpwODAS+U2tVkqQFbp6Bn5nXVpNTgc3qLUeSVJeOXKVzHnO5Aasay5ckdRMdGdK5tml6YWB7GuP4kqRupCNDOmOb5yPiEuD22iqSJNWiI0f4rX0MGLqgC3mfN2fU3oU0PyK6ugJp/nRkDH867x3Df47GnbeSpG6kI0M6i3VGIZKkes3zTtuIuLEjbZKkD7f2noe/MLAIsGREDAJaRi4HAst1Qm2SpAWovSGdbwGHAMsCd/Nu4E8Dzqi5LknSAtbe8/B/Dvw8Ig7KzNM7sSZJUg068rTM2RGxRMtMRAyKiP1rrEmSVIOOBP43M/PVlpnMfAX4Zn0lSZLq0JHA7x3x7q0mEdEb6FdfSZKkOnTkTts/AmMi4lfV/LeA6+srSZJUh44E/hHAKGDfav5eYOnaKpIk1WKeQzqZORv4K43vsv0PGl9v+EC9ZUmSFrT2brxaFdil+nkRGAOQmX4JiiR1Q+0N6TwI3AZsnZmPAkSEX20oSd1Ue0M6OwBTgJsi4jcR8XnevdtWktTNtBn4mXllZu4MrA7cROMxC0Mj4pcR8YXOKlCStGB05KTta5l5cWZuAywP/A2fhy9J3U5HbryaIzNfycxfZ+bn6ypIklSPDxT4kqTuy8CXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKkSfOnYaEecB2cbizMy96+hXktS2WgIfuHYubR8BvgP0rqlPSVI7agn8zBzbMh0Rw4EjgU2AE4Fz6uhTktS+2sbwI2L1iLgIuAa4HRiRmb/MzLfr6lOS1La6xvAvB9YDTqYxjDMLGBgRAGTmy3X0K0lqW11j+J+kcdL2MOBQIJqWJTC8pn4lSW2oawx/pTr2K0maf3Ud4RMR/YDdgDWqpsnAxZn5Vl19SpLaVstJ24gYAdwPbAo8Vf1sCkyOiDXa3lKSVJe6jvBPB/bLzD81N0bE5sAZwGY19StJakNdl2Uu1zrsATLzf4Cla+pTktSOugK/V0Qs1LoxIhamxvMGkqS21RX4FwBjI2LFloaIWAm4DLiwpj4lSe2o67LM4yPiQOC2iFiExnX4M4CTMvP0Ovos2fLDluDs477O0CGLkQnnjr2DMy+5mR02X4ej9t2K1VcexsZ7nMQ99z8FwOc+tTrHffvL9Ovbh7dnvsORp17JLeMf7uJXoRKccOxo7rz9VgYNGswFl10JwLm/OpNrrhzLEoMGATBq/4PZ8DObdGWZPVZtwyuZeQZwRkQsVs1Pr6uv0r0zazbfO+X3THzwGQYsshB3XnwEN/71QSb/41l2PvQ3nDF6l/es/9KrM9jxkF8x5YWpjFhlGa75xQGs8sXRXVS9SrLlNtuxw0678qOjj3xP+9d23YNd9vhGF1VVjrouy9ymZTinCvrvRMSkiLg6Ilauo8+SPffiNCY++AwAM15/iwcff45ll1qChx7/F488+fz71p/00DNMeWEqAPf/YwoLL9SXfn09taL6jVx3fQYOXLyryyhWXWP4PwJeAIiIrYHdgb2Aq4GzaupTwArLDGbkassz/u9PdGj97TcfycQHn+btme/UW5jUjt9fdgl77rw9Jxw7munTpnZ1OT1WXYGfmfl6Nb0DcE5m3p2ZZwNLtbVRRIyKiAkRMeGdFyfXVFrPtWj/flxy0j4cftJYpr/25jzX//jwpTn+29ty4PGXdkJ10txtt+NOXHrl9Zx38ViGLLkUZ/zsp11dUo9VV+BHRAyIiF7A54Ebm5Yt3NZGmfnrzFw/M9fvs6Q35H4Qffr04pKTvsmY6ydw1Z8nzXP95YYuwZhTRrHPf13I48+82AkVSnM3eMiS9O7dm169erHN9jvywOS/d3VJPVZdgX8qMBGYADyQmRMAImIdYEpNfRbtrGN246HHn+O0i/48z3UXH9Cf35++L/912lX8ZdJjnVCd1LYXX3xhzvStN93Iyqt8tAur6dkis62vnv03dxyxHDAUmJSZs6u2ZYC+mfnUvLbvv86B9RTWA200cjg3nvdd7nv4n8yu/n8ec8bVLNS3D6cc8VWWHDSAV6e/wb0P/ZMvH3AmR+zzRQ7f6ws8+tS7/9C22e8MXnhlRle9hG7lyVt/1tUldFs/OPJw/nb3eKa++iqDhwxhr1H787e7x/Poww9BwDLLLMdhRx3Dkku2OfKreRi6WN9oa1ktgR8Ru2fmRdX0pzPzjqZlB1aXbLbLwNeHlYGvD7P2Ar+uIZ3vNk23vtFqr5r6lCS1o7aTtm1Mz21ektQJarsss43puc1LkjpBXbdXrh4R99I4ml+lmqaa9/tsJakL1BX4H69pv5Kk+VTX0zKfnFt7dSPWLsBcl0uS6lPXw9MGRsT3I+KMiPhCNBwEPAZ8rY4+JUntq2tI50LgFeAvwD7AkTTG77fLzIk19SlJakddgT88M9cCiIizaTxOYYXMnPcTvSRJtajrssyZLROZOQt4xrCXpK5V1xH+2hExjXdvsurfNJ+ZObCmfiVJbajrKp3edexXkjT/agn8iFgY2Bf4KHAvcG5m+pVKktSF6hrDPx9YH7gP2Ao4uaZ+JEkdVNcY/oimq3TOAe6qqR9JUgd1xlU6DuVI0odA3VfpQOPKHK/SkaQu5lU6klSIuoZ0JEkfMga+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEJEZnZ1DeoEETEqM3/d1XVIrfne7Dwe4ZdjVFcXILXB92YnMfAlqRAGviQVwsAvh2Ok+rDyvdlJPGkrSYXwCF+SCmHg9wARsXREXBoR/4iIuyPiDxGxarXskIh4MyIWb1p/04iYGhETI+LBiDip66pXdxQRs6r3z98j4vKIWKRVe8vP96r2myPioYiYFBHjI2Jk0772ioj7IuLean/bVu0REaMj4pGIeDgiboqINZq2eyIixjbN7xgRv+20X0I3ZOB3cxERwBXAzZm5SmauB3wfGFatsgswHtih1aa3ZeZIYB1g64j4dGfVrB7hjcwcmZlrAm8D+7Zqb/k5sWmb3TJzbeAXwE8BImJ54CjgM5n5CWAD4N5q/QOAjYC1M3NV4ATg6ohYuGmf60XEiLpeZE9j4Hd/mwEzM/OslobMnJSZt0XEKsAAYDSN4H+fzHwDmAgs1xnFqke6DfjoB1j/L7z7fhsKTAdmAGTmjMx8vFp2BHBgZr5eLbsBuBPYrWlfJ9P4g6EOMPC7vzWBu9tYtjNwKY1/kKtFxLDWK0TEIOBjwK21VageKyL6AFsC91VN/VsN6ew0l822AK6spicB/wIej4jzImKbar8DgUUz87FW204A1miavwxYNyI+yB+cYvXp6gJUq12A7TNzdjXW+VXgjGrZxhExiUbYn5qZz3VVkeqW+kfExGr6NuCcavqNaqhwbn4XEf1ofOocCZCZsyJiC+CTwOeBn0XEesApHaxjFo3hoe8D13/wl1EWj/C7v8nAeq0bI2ItGmH+p4h4gsbRfvOwzm3VeOoawN7NJ9GkDmgeqz8oM9/uwDa7AcOB84HTWxqz4a7MPIHG+/QrmTkNeC0ihrfax3o03vPNLgQ2AT4yvy+mFAZ+9/dnYKGImPM8koj4BHAa8IPMXKn6WRZYNiJWbN64Gi89kcZ4qVSrbNz481/ABhGxekQsGxHrNq0yEniymv4pcFpE9AeIiM2BzwAXt9rnTOBnwHfqrr+7M/C7ueof0PbA5tVlmZNpXM2wKY2rd5pdQeMIqrWzgE0iYqX6KlUhWo/hn9h6hepCgZOBw4G+wEnV5cETgZ2Ag6tVT6dxhdl9EfEQjT8U21bbt3YODlHPk3faSlIhPMKXpEIY+JJUCANfkgph4EtSIQx8SSqEga8eq60nOs7nvn4bETtW02e398Cu6mmkG81HH09ExJLzW6M0Lwa+erK2nugIzHkOzAeWmftk5v3trLIpjac8Sh8qBr5KcRvw0ero+7aIuBq4PyJ6R8RPq2e03xsR34I5z2I/o3qG+//QeKoj1bKbI2L9anqLiLines77jdXNa/sC36k+XWwcEUtFxNiqj/Etj6KOiCERcUNETI6Is4Ho3F+JSuOdaerxmp7o+MeqaV1gzcx8vHokxdTM/GRELATcERE30PiegNWAETS+W+B+4NxW+10K+A2wSbWvwZn5ckScBczIzJOq9S4GfpaZt0fECsA44OPAMcDtmfnDiPgSsHetvwgVz8BXTza3JzpuBNzV9Mz1LwCfaBmfBxan8dC5TYBLMnMW8GxE/Hku+98AuLVlX5n5cht1bA6MaHxXDQADI2JA1ccO1bbXRcQr8/k6pQ4x8NWTve9RvVXovtbcBByUmeNarbfVAqyjF7BBZr45l1qkTuMYvko3DtgvIvoCRMSqEbEojS+E2aka41+GxjeLtfa/NB46t3K17eCqfTqwWNN6NwAHtcw0PYr6VmDXqm1LYNACe1XSXBj4Kt3ZNMbn74mIvwO/ovHJ9wrgkWrZBTS+lu89MvMFYBTw++rLZMZUi64Btm85aQt8G1i/Oil8P+9eLXQsjT8Yk2kM7TxV02uUAJ+WKUnF8Ahfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVIj/A9drLaFc1+c0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "----------------------\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         CAR       0.49      0.93      0.64       221\n",
            "      PERSON       0.48      0.07      0.12       227\n",
            "\n",
            "    accuracy                           0.49       448\n",
            "   macro avg       0.49      0.50      0.38       448\n",
            "weighted avg       0.49      0.49      0.38       448\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npge8SMEd_dI"
      },
      "source": [
        "#Prediction on a new image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "dz29-a4VN6JT",
        "outputId": "0bb4de34-49de-4932-fd1d-56227aa1b2c1"
      },
      "source": [
        "#Path to the image to see if the model predicts the correct class\n",
        "path=\"/content/myphoto.jpeg\"\n",
        "img=load_img(path,target_size=(224,224))\n",
        "\n",
        "i=img_to_array(img)\n",
        "i=preprocess_input(i)\n",
        "\n",
        "input_arr=np.array([i])\n",
        "input_arr.shape\n",
        "\n",
        "pred=np.argmax(model.predict(input_arr))\n",
        "\n",
        "if pred==0:\n",
        "  print('The image is of a car.')\n",
        "else:\n",
        "  print('The image is of a person.')\n",
        "\n",
        "#To display an image\n",
        "plt.imshow(input_arr[0])\n",
        "plt.title('input image')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-fd3186bd0207>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0minput_arr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocess_input' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh6lw2-tolaH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}